{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "# optimizers\n",
    "from torch.optim import Adam\n",
    "import os\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 28, 28)\n",
      "mps\n"
     ]
    }
   ],
   "source": [
    "class Args:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.__dict__)\n",
    "\n",
    "args = Args(\n",
    "    n_epochs=200,\n",
    "    batch_size=64,\n",
    "    lr=0.0002,\n",
    "    b1=0.5,\n",
    "    b2=0.999,\n",
    "    latent_dim=100,\n",
    "    img_size=28,\n",
    "    channels=1, \n",
    ")\n",
    "\n",
    "img_shape = (args.channels, args.img_size, args.img_size)\n",
    "\n",
    "print(img_shape)\n",
    "\n",
    "def get_device():\n",
    "    # m1 chip\n",
    "    return torch.device(\"mps\")\n",
    "\n",
    "device = get_device()\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        def block(in_feat, out_feat, normalize=True):\n",
    "            layers = [nn.Linear(in_feat, out_feat)]\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "        self.model = nn.Sequential(\n",
    "            *block(args.latent_dim, 128, normalize=False),\n",
    "            *block(128, 256),\n",
    "            *block(256, 512),\n",
    "            *block(512, 1024),\n",
    "            nn.Linear(1024, int(np.prod(img_shape))),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self,z):\n",
    "        img = self.model(z)\n",
    "        img = img.view(img.size(0), *img_shape)\n",
    "        return img\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        def block(in_feat, out_feat):\n",
    "            layers = [nn.Linear(in_feat, out_feat)]\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *block(int(np.prod(img_shape)), 512),\n",
    "            *block(512, 256),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        img_flat = img.view(img.size(0),-1)\n",
    "        validity = self.model(img_flat)\n",
    "        return validity\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "adversarial_loss = nn.BCELoss().to(device)\n",
    "\n",
    "generator = Generator().to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "\n",
    "optim_G = Adam(generator.parameters(), lr=args.lr, betas=(args.b1, args.b2))\n",
    "optim_D = Adam(discriminator.parameters(), lr=args.lr, betas=(args.b1, args.b2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loader():\n",
    "    transform = transforms.Compose([transforms.Resize(args.img_size), transforms.ToTensor()])\n",
    "    dataset = datasets.MNIST(\"../../data/mnist\", train=True, download=True, transform=transform)\n",
    "    return DataLoader(\n",
    "        dataset, \n",
    "        batch_size=args.batch_size, \n",
    "        shuffle=True,\n",
    "        num_workers=8,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "data_loader = get_data_loader()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/200] [Avg D loss: 0.5453] [Avg G loss: 0.9380]\n",
      "[Epoch 2/200] [Avg D loss: 0.4652] [Avg G loss: 1.2251]\n",
      "[Epoch 3/200] [Avg D loss: 0.4145] [Avg G loss: 1.4570]\n",
      "[Epoch 4/200] [Avg D loss: 0.3982] [Avg G loss: 1.5423]\n",
      "[Epoch 5/200] [Avg D loss: 0.3748] [Avg G loss: 1.6742]\n",
      "[Epoch 6/200] [Avg D loss: 0.3599] [Avg G loss: 1.7638]\n",
      "[Epoch 7/200] [Avg D loss: 0.3402] [Avg G loss: 1.8677]\n",
      "[Epoch 8/200] [Avg D loss: 0.3238] [Avg G loss: 1.9632]\n",
      "[Epoch 9/200] [Avg D loss: 0.3050] [Avg G loss: 2.0596]\n",
      "[Epoch 10/200] [Avg D loss: 0.2819] [Avg G loss: 2.2200]\n",
      "[Epoch 11/200] [Avg D loss: 0.2707] [Avg G loss: 2.3404]\n",
      "[Epoch 12/200] [Avg D loss: 0.2665] [Avg G loss: 2.3764]\n",
      "[Epoch 13/200] [Avg D loss: 0.2523] [Avg G loss: 2.4558]\n",
      "[Epoch 14/200] [Avg D loss: 0.2424] [Avg G loss: 2.5214]\n",
      "[Epoch 15/200] [Avg D loss: 0.2375] [Avg G loss: 2.6359]\n",
      "[Epoch 16/200] [Avg D loss: 0.2384] [Avg G loss: 2.6574]\n",
      "[Epoch 17/200] [Avg D loss: 0.2242] [Avg G loss: 2.7053]\n",
      "[Epoch 18/200] [Avg D loss: 0.2149] [Avg G loss: 2.8389]\n",
      "[Epoch 19/200] [Avg D loss: 0.2135] [Avg G loss: 2.8923]\n",
      "[Epoch 20/200] [Avg D loss: 0.2037] [Avg G loss: 2.9030]\n",
      "[Epoch 21/200] [Avg D loss: 0.2035] [Avg G loss: 3.0392]\n",
      "[Epoch 22/200] [Avg D loss: 0.2051] [Avg G loss: 3.0096]\n",
      "[Epoch 23/200] [Avg D loss: 0.1892] [Avg G loss: 3.1746]\n",
      "[Epoch 24/200] [Avg D loss: 0.1856] [Avg G loss: 3.1191]\n",
      "[Epoch 25/200] [Avg D loss: 0.1862] [Avg G loss: 3.1378]\n",
      "[Epoch 26/200] [Avg D loss: 0.1778] [Avg G loss: 3.2530]\n",
      "[Epoch 27/200] [Avg D loss: 0.1765] [Avg G loss: 3.2527]\n",
      "[Epoch 28/200] [Avg D loss: 0.1692] [Avg G loss: 3.2858]\n",
      "[Epoch 29/200] [Avg D loss: 0.1706] [Avg G loss: 3.3972]\n",
      "[Epoch 30/200] [Avg D loss: 0.1753] [Avg G loss: 3.4697]\n",
      "[Epoch 31/200] [Avg D loss: 0.1606] [Avg G loss: 3.5072]\n",
      "[Epoch 32/200] [Avg D loss: 0.1662] [Avg G loss: 3.4680]\n",
      "[Epoch 33/200] [Avg D loss: 0.1526] [Avg G loss: 3.6306]\n",
      "[Epoch 34/200] [Avg D loss: 0.1546] [Avg G loss: 3.5683]\n",
      "[Epoch 35/200] [Avg D loss: 0.1541] [Avg G loss: 3.7051]\n",
      "[Epoch 36/200] [Avg D loss: 0.1515] [Avg G loss: 3.5983]\n",
      "[Epoch 37/200] [Avg D loss: 0.1504] [Avg G loss: 3.6421]\n",
      "[Epoch 38/200] [Avg D loss: 0.1425] [Avg G loss: 3.7525]\n",
      "[Epoch 39/200] [Avg D loss: 0.1375] [Avg G loss: 3.7885]\n",
      "[Epoch 40/200] [Avg D loss: 0.1399] [Avg G loss: 3.8763]\n",
      "[Epoch 41/200] [Avg D loss: 0.1353] [Avg G loss: 3.9692]\n",
      "[Epoch 42/200] [Avg D loss: 0.1434] [Avg G loss: 4.0029]\n",
      "[Epoch 43/200] [Avg D loss: 0.1322] [Avg G loss: 3.9773]\n",
      "[Epoch 44/200] [Avg D loss: 0.1256] [Avg G loss: 4.1983]\n",
      "[Epoch 45/200] [Avg D loss: 0.1283] [Avg G loss: 4.2014]\n",
      "[Epoch 46/200] [Avg D loss: 0.1265] [Avg G loss: 4.2995]\n",
      "[Epoch 47/200] [Avg D loss: 0.1199] [Avg G loss: 4.2061]\n",
      "[Epoch 48/200] [Avg D loss: 0.1173] [Avg G loss: 4.3790]\n",
      "[Epoch 49/200] [Avg D loss: 0.1248] [Avg G loss: 4.4320]\n",
      "[Epoch 50/200] [Avg D loss: 0.1135] [Avg G loss: 4.3881]\n",
      "[Epoch 51/200] [Avg D loss: 0.1147] [Avg G loss: 4.3499]\n",
      "[Epoch 52/200] [Avg D loss: 0.1102] [Avg G loss: 4.6833]\n",
      "[Epoch 53/200] [Avg D loss: 0.1139] [Avg G loss: 4.7148]\n",
      "[Epoch 54/200] [Avg D loss: 0.1117] [Avg G loss: 4.5412]\n",
      "[Epoch 55/200] [Avg D loss: 0.1150] [Avg G loss: 4.5370]\n",
      "[Epoch 56/200] [Avg D loss: 0.1073] [Avg G loss: 4.5988]\n",
      "[Epoch 57/200] [Avg D loss: 0.1105] [Avg G loss: 4.6477]\n",
      "[Epoch 58/200] [Avg D loss: 0.1101] [Avg G loss: 4.4300]\n",
      "[Epoch 59/200] [Avg D loss: 0.1049] [Avg G loss: 4.5612]\n",
      "[Epoch 60/200] [Avg D loss: 0.1114] [Avg G loss: 4.7488]\n",
      "[Epoch 61/200] [Avg D loss: 0.1083] [Avg G loss: 4.5705]\n",
      "[Epoch 62/200] [Avg D loss: 0.1142] [Avg G loss: 4.7860]\n",
      "[Epoch 63/200] [Avg D loss: 0.1014] [Avg G loss: 4.5917]\n",
      "[Epoch 64/200] [Avg D loss: 0.0962] [Avg G loss: 4.6129]\n",
      "[Epoch 65/200] [Avg D loss: 0.0968] [Avg G loss: 4.7590]\n",
      "[Epoch 66/200] [Avg D loss: 0.1023] [Avg G loss: 4.7258]\n",
      "[Epoch 67/200] [Avg D loss: 0.0980] [Avg G loss: 4.7609]\n",
      "[Epoch 68/200] [Avg D loss: 0.1003] [Avg G loss: 5.0815]\n",
      "[Epoch 69/200] [Avg D loss: 0.1017] [Avg G loss: 4.9915]\n",
      "[Epoch 70/200] [Avg D loss: 0.0914] [Avg G loss: 4.8450]\n",
      "[Epoch 71/200] [Avg D loss: 0.0880] [Avg G loss: 4.9086]\n",
      "[Epoch 72/200] [Avg D loss: 0.0986] [Avg G loss: 5.0453]\n",
      "[Epoch 73/200] [Avg D loss: 0.0898] [Avg G loss: 5.0269]\n",
      "[Epoch 74/200] [Avg D loss: 0.0953] [Avg G loss: 4.8359]\n",
      "[Epoch 75/200] [Avg D loss: 0.0993] [Avg G loss: 5.4368]\n",
      "[Epoch 76/200] [Avg D loss: 0.0980] [Avg G loss: 5.0105]\n",
      "[Epoch 77/200] [Avg D loss: 0.0912] [Avg G loss: 4.8397]\n",
      "[Epoch 78/200] [Avg D loss: 0.0856] [Avg G loss: 5.0821]\n",
      "[Epoch 79/200] [Avg D loss: 0.0902] [Avg G loss: 4.8777]\n",
      "[Epoch 80/200] [Avg D loss: 0.0849] [Avg G loss: 5.1537]\n",
      "[Epoch 81/200] [Avg D loss: 0.0838] [Avg G loss: 5.3485]\n",
      "[Epoch 82/200] [Avg D loss: 0.0920] [Avg G loss: 5.2500]\n",
      "[Epoch 83/200] [Avg D loss: 0.0896] [Avg G loss: 5.1681]\n",
      "[Epoch 84/200] [Avg D loss: 0.0866] [Avg G loss: 5.0767]\n",
      "[Epoch 85/200] [Avg D loss: 0.0869] [Avg G loss: 5.1040]\n",
      "[Epoch 86/200] [Avg D loss: 0.0899] [Avg G loss: 5.2652]\n",
      "[Epoch 87/200] [Avg D loss: 0.0815] [Avg G loss: 4.9816]\n",
      "[Epoch 88/200] [Avg D loss: 0.0834] [Avg G loss: 5.4614]\n",
      "[Epoch 89/200] [Avg D loss: 0.0877] [Avg G loss: 5.2932]\n",
      "[Epoch 90/200] [Avg D loss: 0.0869] [Avg G loss: 5.2046]\n",
      "[Epoch 91/200] [Avg D loss: 0.0840] [Avg G loss: 5.1105]\n",
      "[Epoch 92/200] [Avg D loss: 0.0882] [Avg G loss: 5.2739]\n",
      "[Epoch 93/200] [Avg D loss: 0.0803] [Avg G loss: 5.1961]\n",
      "[Epoch 94/200] [Avg D loss: 0.0900] [Avg G loss: 5.3142]\n",
      "[Epoch 95/200] [Avg D loss: 0.0901] [Avg G loss: 5.6916]\n",
      "[Epoch 96/200] [Avg D loss: 0.0839] [Avg G loss: 5.2966]\n",
      "[Epoch 97/200] [Avg D loss: 0.0809] [Avg G loss: 5.3894]\n",
      "[Epoch 98/200] [Avg D loss: 0.0813] [Avg G loss: 5.3598]\n",
      "[Epoch 99/200] [Avg D loss: 0.0799] [Avg G loss: 5.2988]\n",
      "[Epoch 100/200] [Avg D loss: 0.0785] [Avg G loss: 5.3754]\n",
      "[Epoch 101/200] [Avg D loss: 0.0744] [Avg G loss: 5.4456]\n",
      "[Epoch 102/200] [Avg D loss: 0.0692] [Avg G loss: 5.4836]\n",
      "[Epoch 103/200] [Avg D loss: 0.0811] [Avg G loss: 5.5785]\n",
      "[Epoch 104/200] [Avg D loss: 0.0721] [Avg G loss: 5.5002]\n",
      "[Epoch 105/200] [Avg D loss: 0.0791] [Avg G loss: 5.4398]\n",
      "[Epoch 106/200] [Avg D loss: 0.0739] [Avg G loss: 5.3128]\n",
      "[Epoch 107/200] [Avg D loss: 0.0806] [Avg G loss: 5.7509]\n",
      "[Epoch 108/200] [Avg D loss: 0.0764] [Avg G loss: 5.7185]\n",
      "[Epoch 109/200] [Avg D loss: 0.0787] [Avg G loss: 5.4888]\n",
      "[Epoch 110/200] [Avg D loss: 0.0754] [Avg G loss: 5.7478]\n",
      "[Epoch 111/200] [Avg D loss: 0.0734] [Avg G loss: 5.4934]\n",
      "[Epoch 112/200] [Avg D loss: 0.0750] [Avg G loss: 5.6589]\n",
      "[Epoch 113/200] [Avg D loss: 0.0702] [Avg G loss: 5.4906]\n",
      "[Epoch 114/200] [Avg D loss: 0.0681] [Avg G loss: 5.6868]\n",
      "[Epoch 115/200] [Avg D loss: 0.0713] [Avg G loss: 5.5837]\n",
      "[Epoch 116/200] [Avg D loss: 0.0790] [Avg G loss: 6.2155]\n",
      "[Epoch 117/200] [Avg D loss: 0.0772] [Avg G loss: 5.9469]\n",
      "[Epoch 118/200] [Avg D loss: 0.0723] [Avg G loss: 5.4045]\n",
      "[Epoch 119/200] [Avg D loss: 0.0683] [Avg G loss: 5.7057]\n",
      "[Epoch 120/200] [Avg D loss: 0.0650] [Avg G loss: 5.9491]\n",
      "[Epoch 121/200] [Avg D loss: 0.0681] [Avg G loss: 5.9317]\n",
      "[Epoch 122/200] [Avg D loss: 0.0708] [Avg G loss: 6.1007]\n",
      "[Epoch 123/200] [Avg D loss: 0.0664] [Avg G loss: 5.7557]\n",
      "[Epoch 124/200] [Avg D loss: 0.0814] [Avg G loss: 5.9862]\n",
      "[Epoch 125/200] [Avg D loss: 0.0654] [Avg G loss: 5.8858]\n",
      "[Epoch 126/200] [Avg D loss: 0.0712] [Avg G loss: 5.8189]\n",
      "[Epoch 127/200] [Avg D loss: 0.0696] [Avg G loss: 5.9124]\n",
      "[Epoch 128/200] [Avg D loss: 0.0660] [Avg G loss: 5.9054]\n",
      "[Epoch 129/200] [Avg D loss: 0.0736] [Avg G loss: 6.0372]\n",
      "[Epoch 130/200] [Avg D loss: 0.0687] [Avg G loss: 5.7593]\n",
      "[Epoch 131/200] [Avg D loss: 0.0708] [Avg G loss: 5.9663]\n",
      "[Epoch 132/200] [Avg D loss: 0.0657] [Avg G loss: 5.8382]\n",
      "[Epoch 133/200] [Avg D loss: 0.0642] [Avg G loss: 5.7386]\n",
      "[Epoch 134/200] [Avg D loss: 0.0713] [Avg G loss: 6.6254]\n",
      "[Epoch 135/200] [Avg D loss: 0.0713] [Avg G loss: 6.1066]\n",
      "[Epoch 136/200] [Avg D loss: 0.0685] [Avg G loss: 6.1448]\n",
      "[Epoch 137/200] [Avg D loss: 0.0706] [Avg G loss: 6.0598]\n",
      "[Epoch 138/200] [Avg D loss: 0.0599] [Avg G loss: 6.0252]\n",
      "[Epoch 139/200] [Avg D loss: 0.0628] [Avg G loss: 6.2965]\n",
      "[Epoch 140/200] [Avg D loss: 0.0605] [Avg G loss: 6.1657]\n",
      "[Epoch 141/200] [Avg D loss: 0.0610] [Avg G loss: 6.0620]\n",
      "[Epoch 142/200] [Avg D loss: 0.0734] [Avg G loss: 5.8702]\n",
      "[Epoch 143/200] [Avg D loss: 0.0694] [Avg G loss: 6.1203]\n",
      "[Epoch 144/200] [Avg D loss: 0.0637] [Avg G loss: 5.9949]\n",
      "[Epoch 145/200] [Avg D loss: 0.0694] [Avg G loss: 6.2497]\n",
      "[Epoch 146/200] [Avg D loss: 0.0648] [Avg G loss: 6.0051]\n",
      "[Epoch 147/200] [Avg D loss: 0.0645] [Avg G loss: 5.9441]\n",
      "[Epoch 148/200] [Avg D loss: 0.0635] [Avg G loss: 6.1530]\n",
      "[Epoch 149/200] [Avg D loss: 0.0667] [Avg G loss: 6.3382]\n",
      "[Epoch 150/200] [Avg D loss: 0.0599] [Avg G loss: 6.0480]\n",
      "[Epoch 151/200] [Avg D loss: 0.0649] [Avg G loss: 6.2752]\n",
      "[Epoch 152/200] [Avg D loss: 0.0675] [Avg G loss: 6.0788]\n",
      "[Epoch 153/200] [Avg D loss: 0.0713] [Avg G loss: 5.8502]\n",
      "[Epoch 154/200] [Avg D loss: 0.0627] [Avg G loss: 5.7338]\n",
      "[Epoch 155/200] [Avg D loss: 0.0642] [Avg G loss: 6.2975]\n",
      "[Epoch 156/200] [Avg D loss: 0.0648] [Avg G loss: 5.8581]\n",
      "[Epoch 157/200] [Avg D loss: 0.0594] [Avg G loss: 5.5271]\n",
      "[Epoch 158/200] [Avg D loss: 0.0617] [Avg G loss: 6.3041]\n",
      "[Epoch 159/200] [Avg D loss: 0.0661] [Avg G loss: 6.3948]\n",
      "[Epoch 160/200] [Avg D loss: 0.0648] [Avg G loss: 6.1156]\n",
      "[Epoch 161/200] [Avg D loss: 0.0659] [Avg G loss: 6.2466]\n",
      "[Epoch 162/200] [Avg D loss: 0.0740] [Avg G loss: 6.3305]\n",
      "[Epoch 163/200] [Avg D loss: 0.0664] [Avg G loss: 6.2656]\n",
      "[Epoch 164/200] [Avg D loss: 0.0653] [Avg G loss: 6.3947]\n",
      "[Epoch 165/200] [Avg D loss: 0.0625] [Avg G loss: 5.9612]\n",
      "[Epoch 166/200] [Avg D loss: 0.0639] [Avg G loss: 6.1803]\n",
      "[Epoch 167/200] [Avg D loss: 0.0631] [Avg G loss: 5.8580]\n",
      "[Epoch 168/200] [Avg D loss: 0.0650] [Avg G loss: 6.0554]\n",
      "[Epoch 169/200] [Avg D loss: 0.0697] [Avg G loss: 6.0564]\n",
      "[Epoch 170/200] [Avg D loss: 0.0656] [Avg G loss: 6.3428]\n",
      "[Epoch 171/200] [Avg D loss: 0.0605] [Avg G loss: 6.0429]\n",
      "[Epoch 172/200] [Avg D loss: 0.0608] [Avg G loss: 6.0874]\n",
      "[Epoch 173/200] [Avg D loss: 0.0674] [Avg G loss: 6.3335]\n",
      "[Epoch 174/200] [Avg D loss: 0.0678] [Avg G loss: 6.1947]\n",
      "[Epoch 175/200] [Avg D loss: 0.0616] [Avg G loss: 6.0566]\n",
      "[Epoch 176/200] [Avg D loss: 0.0701] [Avg G loss: 5.8884]\n",
      "[Epoch 177/200] [Avg D loss: 0.0582] [Avg G loss: 5.6860]\n",
      "[Epoch 178/200] [Avg D loss: 0.0659] [Avg G loss: 6.1355]\n",
      "[Epoch 179/200] [Avg D loss: 0.0745] [Avg G loss: 6.3438]\n",
      "[Epoch 180/200] [Avg D loss: 0.0623] [Avg G loss: 5.7252]\n",
      "[Epoch 181/200] [Avg D loss: 0.0656] [Avg G loss: 6.1324]\n",
      "[Epoch 182/200] [Avg D loss: 0.0654] [Avg G loss: 5.7216]\n",
      "[Epoch 183/200] [Avg D loss: 0.0688] [Avg G loss: 6.1400]\n",
      "[Epoch 184/200] [Avg D loss: 0.0619] [Avg G loss: 6.3236]\n",
      "[Epoch 185/200] [Avg D loss: 0.0624] [Avg G loss: 5.9011]\n",
      "[Epoch 186/200] [Avg D loss: 0.0610] [Avg G loss: 5.8881]\n",
      "[Epoch 187/200] [Avg D loss: 0.0598] [Avg G loss: 6.0266]\n",
      "[Epoch 188/200] [Avg D loss: 0.0575] [Avg G loss: 5.7491]\n",
      "[Epoch 189/200] [Avg D loss: 0.0648] [Avg G loss: 6.5586]\n",
      "[Epoch 190/200] [Avg D loss: 0.0647] [Avg G loss: 6.1524]\n",
      "[Epoch 191/200] [Avg D loss: 0.0631] [Avg G loss: 6.2591]\n",
      "[Epoch 192/200] [Avg D loss: 0.0671] [Avg G loss: 6.9683]\n",
      "[Epoch 193/200] [Avg D loss: 0.0690] [Avg G loss: 6.2076]\n",
      "[Epoch 194/200] [Avg D loss: 0.0602] [Avg G loss: 5.6607]\n",
      "[Epoch 195/200] [Avg D loss: 0.0581] [Avg G loss: 5.8715]\n",
      "[Epoch 196/200] [Avg D loss: 0.0621] [Avg G loss: 6.1341]\n",
      "[Epoch 197/200] [Avg D loss: 0.0617] [Avg G loss: 5.7449]\n",
      "[Epoch 198/200] [Avg D loss: 0.0667] [Avg G loss: 6.1565]\n",
      "[Epoch 199/200] [Avg D loss: 0.0672] [Avg G loss: 6.1370]\n",
      "[Epoch 200/200] [Avg D loss: 0.0638] [Avg G loss: 5.9524]\n"
     ]
    }
   ],
   "source": [
    "try:  \n",
    "    os.makedirs(\"../../images\", exist_ok=True)\n",
    "    \n",
    "    Tensor = torch.FloatTensor\n",
    "    for epoch in range(args.n_epochs):\n",
    "        total_g_loss = 0\n",
    "        total_d_loss = 0\n",
    "        batch_count = 0\n",
    "        for i, (imgs, _) in enumerate(data_loader):\n",
    "            valid = Tensor(imgs.size(0), 1).fill_(1.0).to(device)\n",
    "            fake = Tensor(imgs.size(0), 1).fill_(0.0).to(device)\n",
    "\n",
    "            real_imgs = imgs.to(device)\n",
    "\n",
    "            optim_G.zero_grad()\n",
    "\n",
    "            z = torch.randn(\n",
    "                imgs.size(0),\n",
    "                args.latent_dim,\n",
    "                ).to(device)\n",
    "            \n",
    "            gen_imgs = generator(z)\n",
    "\n",
    "            d_gen_imgs = discriminator(gen_imgs)\n",
    "            # print(valid.shape)\n",
    "            # print(d_gen_imgs.shape)\n",
    "\n",
    "\n",
    "            g_loss = adversarial_loss(d_gen_imgs, valid)\n",
    "\n",
    "            g_loss.backward()\n",
    "            optim_G.step()\n",
    "\n",
    "            optim_D.zero_grad()\n",
    "            real_loss = adversarial_loss(\n",
    "                discriminator(real_imgs),\n",
    "                valid\n",
    "            )\n",
    "            fake_loss = adversarial_loss(\n",
    "                discriminator(gen_imgs.detach()),\n",
    "                fake\n",
    "            )\n",
    "\n",
    "            d_loss = (real_loss + fake_loss) / 2\n",
    "            d_loss.backward()\n",
    "            optim_D.step()\n",
    "\n",
    "            total_g_loss += g_loss.item()\n",
    "            total_d_loss += d_loss.item()\n",
    "            batch_count += 1        \n",
    "        \n",
    "        save_image(gen_imgs.data, f\"../../images/{epoch+1}.png\")\n",
    "        avg_g_loss = total_g_loss / len(data_loader)\n",
    "        avg_d_loss = total_d_loss / len(data_loader)\n",
    "        print(f\"[Epoch {epoch+1}/{args.n_epochs}] [Avg D loss: {avg_d_loss:.4f}] [Avg G loss: {avg_g_loss:.4f}]\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error in training\")\n",
    "    print(\"z\", z.shape)\n",
    "    print(\"d_gen_imgs\", d_gen_imgs.shape)\n",
    "    print(\"valid\", valid.shape)\n",
    "    print(\"fake\", fake.shape)\n",
    "    raise e\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
